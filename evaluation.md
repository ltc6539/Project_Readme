# xx 省水源保护专家系统

## 项目概述

本项目旨在开发一款智能问答系统，能够针对 xx 省生态环境厅信箱中有关水源保护区的问题，提供准确、全面、权威的回答。

## 评估结果

为评估模型的表现，我们选取了多项指标，包括关键词匹配、BERTScore、召回率（Recall）、准确率（Precision）、BLEU、ROUGE-1，以及基于 ChatGPT 的评分系统。通过这些指标，我们能够量化模型在回答质量、精准性和语义一致性方面的表现。

### 方法与指标

| 方法              | KeyWord | BERTScore | Recall | Precision | BLEU   | ROUGE-1 | ChatGPT |
| ----------------- | ------- | --------- | ------ | --------- | ------ | ------- | ------- |
| 随即回答          | 0.1731  | 0.7047    | 0.7216 | 0.6917    | 0.1270 | 0.1548  | 0.3231  |
| **OpenAI 方法**   |         |           |        |           |        |         |         |
| 基础模型          | 0.3508  | 0.6781    | 0.7221 | 0.6403    | 0.0441 | 0.2230  | 0.7496  |
| 优化提示词        | 0.4379  | 0.7453    | 0.7866 | 0.7109    | 0.1358 | 0.1895  | 0.7900  |
| RAG 方法          | 0.2904  | 0.7014    | 0.7444 | 0.6649    | 0.0703 | 0.2296  | 0.7778  |
| 优化提示词+RAG    | 0.3071  | 0.7414    | 0.7892 | 0.7016    | 0.1470 | 0.1915  | 0.8000  |
| 优化提示词+RAG-v2 | 0.3748  | 0.7482    | 0.7998 | 0.7068    | 0.1535 | 0.2079  | 0.8059  |
| **智谱方法**      |         |           |        |           |        |         |         |
| 基础模型          | 0.3523  | 0.6715    | 0.7193 | 0.6309    | 0.0403 | 0.2246  | 0.7646  |
| 优化提示词        | 0.4301  | 0.7480    | 0.7792 | 0.7231    | 0.1498 | 0.1758  | 0.7807  |
| 优化提示词+RAG-v2 | 0.3356  | 0.7569    | 0.7932 | 0.7274    | 0.1676 | 0.2003  | 0.8000  |

**注：以上指标评分范围均为 0-1，分数越高，模型越好**

## 评估集

- **来源**：xx 省生态环境厅信箱中 26 个关于水源保护区的问题及回复。
- **目的**：评估模型在真实场景中回答用户问题的能力。

## 评估指标说明

为了全面评估模型的表现，我们采用了多种指标：

- **KeyWord**：评估模型回答中与原始回答中特定关键词的精确匹配程度，反映模型引用法规的准确性。能反映模型精准引用法规的能力，但在遇到法规缩写或不同表述时，可能出现偏差。
- **BERTScore**：评估模型回答与原始回答在语义上的相似度。衡量语义相似度，但高分并不一定代表内容准确，尤其在原始回答过于简略时。
- **Recall**：评估模型回答涵盖了原始回答中内容的比例。衡量模型回答的准确度，回答过长可能导致分数降低。
- **Precision**：评估原始回答涵盖了模型回答中内容的比例。衡量模型回答的覆盖范围。
- **BLEU**：评估模型回答中的词组在原始回答中出现的频率。衡量模型回答的准确度，回答过长可能导致分数降低。
- **ROUGE-1**：评估原始回答中的词组在模型回答中出现的频率。衡量模型回答的覆盖范围。
- **ChatGPT 评分**：从内容准确性、法规引用准确性、回答完整性、问题相关性、可操作性、权威性和简洁程度六个维度对模型回答进行综合评分。与人类评判最接近，是评估模型表现的首选指标。

## 技术方法说明

- **随机回答**：使用固定的答案作为所有问题的模型回答，旨在评估在明显错误情况下各指标的表现。
- **基础模型**：直接将问题输入大模型，获得模型的原始回答。
- **优化提示词（Prompt Engineering）**：通过设计更好的提示词，如扮演特定角色、提出明确要求、提供回答思路、控制回答风格等，提升模型的回答质量。
- **RAG 方法（检索增强生成）**：从知识库中检索最相关的 5 个文档段落，与问题一起输入模型，辅助生成更准确的回答。（知识库包含 8 本相关法律法规）
- **优化提示词 + RAG**：同时应用优化提示词和 RAG 方法，进一步提升模型表现。
- **优化提示词 + RAG v2**：改进了向量检索方式，选择了 3 本结构化的法律法规，并使用 Reranker 对检索结果进行优化。

## 主要技术改进及成效

### 1. 基础模型与优化提示词（Prompt Engineering）

在最初的基础模型下，关键词匹配得分为 0.3508，ChatGPT 评分为 0.7496，虽然能够生成一些合理的回答，但整体精度和专业性不足。通过**优化提示词**的方式，我们引导模型更精准地扮演特定角色，并明确给出回答要求，显著提升了模型的表现。优化后，关键词匹配得分提升至 0.4379，提升了**8.71 个百分点**，而 ChatGPT 评分也从 0.7496 提升至 0.7900，提升了**4 个百分点**，显示出在语义一致性和专业性上的明显改善。

### 2. RAG 方法的引入

为了提升模型对领域知识的引用能力，我们引入了**RAG（检索增强生成）**方法，将相关的知识库文档段落与问题一起输入模型。结果表明，RAG 方法在提升语义覆盖和回答准确性方面有显著效果，ChatGPT 评分提升至 0.7778，较基础模型提高了**2.82 个百分点**。此外，召回率从 0.7221 提升至 0.7444，提升了**2.23 个百分点**，表明模型能够更好地捕捉到问题中的关键信息。然而，由于知识库的局限性，关键词匹配得分略有下降。

### 3. 优化提示词+RAG

在 RAG 方法的基础上，结合**优化提示词**技术，我们进一步增强了模型的表现。在这两者结合后，模型的 ChatGPT 评分达到了 0.8000，较单独使用 RAG 时提升了**2.22 个百分点**。BLEU 分数从 0.0703 跃升至 0.1470，几乎翻倍，表明模型在 N-gram 精确匹配方面表现更加突出。这种结合方法确保了模型既能够检索相关知识库内容，又能够以更自然、连贯的方式呈现回答。

### 4. 优化提示词+RAG v2

为了进一步提升性能，我们对 RAG 进行了第二次优化，引入了**RAG v2**版本。该版本通过对法律条目进行更精细的切割，并引入 Reranker 对检索结果进行优化，使得关键词匹配得分从 0.2904 提升至 0.3748，提升了**8.44 个百分点**。此外，ChatGPT 评分进一步提高至 0.8059，较基础 RAG 提升了**2.81 个百分点**，模型在精准性和权威性上的表现得到了显著提升。

### 5. 智谱模型的表现

智谱模型在未调优的情况下，展现出较强的性能，关键词匹配得分为 0.3523，ChatGPT 评分为 0.7646，部分场景下优于 OpenAI 基础模型。经过调优后的智谱 Prompt-RAG v2 模型在所有指标上表现更加均衡，ChatGPT 评分达到了 0.8000，与优化后的 OpenAI 模型相当。然而，由于中文语境下的特殊性，该模型在一些特定场景下的表现仍有进一步优化的空间。

## 总结

通过对多种技术方法的探索和优化，我们成功提升了模型在回答 xx 省水源保护相关问题上的表现。优化提示词和检索增强生成（RAG）方法的结合，显著提高了模型的准确性和可靠性。未来，我们将继续扩大知识库的规模和质量，并对模型进行进一步的优化，以满足实际应用的需求。
